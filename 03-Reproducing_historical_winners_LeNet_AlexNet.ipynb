{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducing LeNet and AlexNet in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous notebooks, I have reproduced example CNN's and simple feed forward NN's on the CIFAR-10 images in Keras. In this notebook, my goal will be to build CNN's with the structure of historically groundbreaking convulutional neural networks. \n",
    "\n",
    "The first historical CNN I will reproduce here will have the structure of LeNet:\n",
    "\n",
    "    INPUT => CONV => RELU => POOL => CONV => RELU => POOL => FC => RELU => FC\n",
    "    \n",
    "The second will be AlexNet, the winner of the 2012 ImageNet Large-Scale Visual Recognition Challenge:\n",
    "    \n",
    "    INPUT => CONV => RELU => POOL => CONV => RELU => POOL => CONV => RELU => CONV => RELU => CONV => RELU => FC => RELU => FC => RELU => FC => SOFTMAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet's structure has existed since 1998, years before CNN's achieved state-of-the-art status recognition in image classification. \n",
    "\n",
    "The disparity between the length of time CNN's have existed and how long they have been recognized as state-of-the-art stems from the availability of processing power to actually put the theoretical frameworks to work and begin building deep, complex structures. LeNet, which was functional as early as 1998, simply was not deep enough to be considered state-of-the-art compared to other methods of image classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (50000, 32, 32, 3), y_train: (50000, 10)\n",
      "X_test: (10000, 32, 32, 3), y_test: (10000, 10)\n",
      "Class labels: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "%run __init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(model, verbose=False):\n",
    "    if verbose:\n",
    "        for l in model.layers:\n",
    "            print (l.name, l.input_shape,'==>',l.output_shape)\n",
    "    \n",
    "    print (model.summary())\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3,3), padding='same', input_shape=X_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(64, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_1 (None, 32, 32, 3) ==> (None, 32, 32, 32)\n",
      "activation_1 (None, 32, 32, 32) ==> (None, 32, 32, 32)\n",
      "max_pooling2d_1 (None, 32, 32, 32) ==> (None, 16, 16, 32)\n",
      "conv2d_2 (None, 16, 16, 32) ==> (None, 14, 14, 64)\n",
      "activation_2 (None, 14, 14, 64) ==> (None, 14, 14, 64)\n",
      "max_pooling2d_2 (None, 14, 14, 64) ==> (None, 7, 7, 64)\n",
      "flatten_1 (None, 7, 7, 64) ==> (None, 3136)\n",
      "dense_1 (None, 3136) ==> (None, 512)\n",
      "activation_3 (None, 512) ==> (None, 512)\n",
      "dense_2 (None, 512) ==> (None, 10)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 1,630,666\n",
      "Trainable params: 1,630,666\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/25\n",
      "50000/50000 [==============================] - 83s - loss: 8.3219 - acc: 0.1046 - val_loss: 8.6844 - val_acc: 0.1178\n",
      "Epoch 2/25\n",
      "50000/50000 [==============================] - 81s - loss: 8.5597 - acc: 0.1166 - val_loss: 8.6844 - val_acc: 0.1178\n",
      "Epoch 3/25\n",
      "50000/50000 [==============================] - 87s - loss: 8.5597 - acc: 0.1166 - val_loss: 8.6844 - val_acc: 0.1178\n",
      "Epoch 4/25\n",
      "50000/50000 [==============================] - 81s - loss: 8.5597 - acc: 0.1166 - val_loss: 8.6844 - val_acc: 0.1178\n",
      "Epoch 5/25\n",
      "50000/50000 [==============================] - 76s - loss: 8.5597 - acc: 0.1166 - val_loss: 8.6844 - val_acc: 0.1178\n",
      "Epoch 6/25\n",
      "50000/50000 [==============================] - 89s - loss: 8.5597 - acc: 0.1166 - val_loss: 8.6844 - val_acc: 0.1178\n",
      "Epoch 7/25\n",
      "50000/50000 [==============================] - 87s - loss: 8.5597 - acc: 0.1166 - val_loss: 8.6844 - val_acc: 0.1178\n",
      "Epoch 8/25\n",
      "50000/50000 [==============================] - 90s - loss: 8.5597 - acc: 0.1166 - val_loss: 8.6844 - val_acc: 0.1178\n",
      "Epoch 9/25\n",
      "50000/50000 [==============================] - 87s - loss: 8.5597 - acc: 0.1166 - val_loss: 8.6844 - val_acc: 0.1178\n",
      "Epoch 10/25\n",
      "50000/50000 [==============================] - 108s - loss: 8.5597 - acc: 0.1166 - val_loss: 8.6844 - val_acc: 0.1178\n",
      "Epoch 11/25\n",
      "50000/50000 [==============================] - 84s - loss: 8.5597 - acc: 0.1166 - val_loss: 8.6844 - val_acc: 0.1178\n",
      "Epoch 12/25\n",
      "50000/50000 [==============================] - 92s - loss: 8.5597 - acc: 0.1166 - val_loss: 8.6844 - val_acc: 0.1178\n",
      "Epoch 13/25\n",
      "50000/50000 [==============================] - 89s - loss: 8.5597 - acc: 0.1166 - val_loss: 8.6844 - val_acc: 0.1178\n",
      "Epoch 14/25\n",
      "50000/50000 [==============================] - 87s - loss: 8.5597 - acc: 0.1166 - val_loss: 8.6844 - val_acc: 0.1178\n",
      "Epoch 15/25\n",
      "50000/50000 [==============================] - 88s - loss: 8.5597 - acc: 0.1166 - val_loss: 8.6844 - val_acc: 0.1178\n",
      "Epoch 16/25\n",
      "50000/50000 [==============================] - 90s - loss: 8.5597 - acc: 0.1166 - val_loss: 8.6844 - val_acc: 0.1178\n",
      "Epoch 17/25\n",
      "50000/50000 [==============================] - 93s - loss: 8.5597 - acc: 0.1166 - val_loss: 8.6844 - val_acc: 0.1178\n",
      "Epoch 18/25\n",
      "50000/50000 [==============================] - 90s - loss: 8.5597 - acc: 0.1166 - val_loss: 8.6844 - val_acc: 0.1178\n",
      "Epoch 19/25\n",
      "50000/50000 [==============================] - 88s - loss: 8.5597 - acc: 0.1166 - val_loss: 8.6844 - val_acc: 0.1178\n",
      "Epoch 20/25\n",
      "50000/50000 [==============================] - 91s - loss: 8.5597 - acc: 0.1166 - val_loss: 8.6844 - val_acc: 0.1178\n",
      "Epoch 21/25\n",
      "50000/50000 [==============================] - 87s - loss: 8.5597 - acc: 0.1166 - val_loss: 8.6844 - val_acc: 0.1178\n",
      "Epoch 22/25\n",
      "50000/50000 [==============================] - 88s - loss: 8.5597 - acc: 0.1166 - val_loss: 8.6844 - val_acc: 0.1178\n",
      "Epoch 23/25\n",
      "50000/50000 [==============================] - 90s - loss: 8.5597 - acc: 0.1166 - val_loss: 8.6844 - val_acc: 0.1178\n",
      "Epoch 24/25\n",
      "50000/50000 [==============================] - 89s - loss: 8.5597 - acc: 0.1166 - val_loss: 8.6844 - val_acc: 0.1178\n",
      "Epoch 25/25\n",
      "50000/50000 [==============================] - 90s - loss: 8.5597 - acc: 0.1166 - val_loss: 8.6844 - val_acc: 0.1178\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f76cc646da0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "              batch_size=32,\n",
    "              epochs=25,\n",
    "              validation_data=(X_test, y_test),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Keras code seeks to implement a model similiar to that used in 2012 called AlexNet. This model was the first CNN to really win when it came to image classification, and hopefully will be able to produce much better results here than I have found so far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3,3), padding='same', input_shape=X_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(32, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(64, (3,3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(64, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(64, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(3072))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_4 (None, 32, 32, 3) ==> (None, 32, 32, 32)\n",
      "activation_5 (None, 32, 32, 32) ==> (None, 32, 32, 32)\n",
      "max_pooling2d_4 (None, 32, 32, 32) ==> (None, 16, 16, 32)\n",
      "batch_normalization_1 (None, 16, 16, 32) ==> (None, 16, 16, 32)\n",
      "conv2d_5 (None, 16, 16, 32) ==> (None, 14, 14, 32)\n",
      "activation_6 (None, 14, 14, 32) ==> (None, 14, 14, 32)\n",
      "max_pooling2d_5 (None, 14, 14, 32) ==> (None, 7, 7, 32)\n",
      "batch_normalization_2 (None, 7, 7, 32) ==> (None, 7, 7, 32)\n",
      "conv2d_6 (None, 7, 7, 32) ==> (None, 7, 7, 64)\n",
      "activation_7 (None, 7, 7, 64) ==> (None, 7, 7, 64)\n",
      "conv2d_7 (None, 7, 7, 64) ==> (None, 5, 5, 64)\n",
      "activation_8 (None, 5, 5, 64) ==> (None, 5, 5, 64)\n",
      "conv2d_8 (None, 5, 5, 64) ==> (None, 3, 3, 64)\n",
      "activation_9 (None, 3, 3, 64) ==> (None, 3, 3, 64)\n",
      "max_pooling2d_6 (None, 3, 3, 64) ==> (None, 1, 1, 64)\n",
      "flatten_2 (None, 1, 1, 64) ==> (None, 64)\n",
      "dense_3 (None, 64) ==> (None, 3072)\n",
      "activation_10 (None, 3072) ==> (None, 3072)\n",
      "dense_4 (None, 3072) ==> (None, 512)\n",
      "activation_11 (None, 512) ==> (None, 512)\n",
      "dense_5 (None, 512) ==> (None, 10)\n",
      "activation_12 (None, 10) ==> (None, 10)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 7, 7, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 5, 5, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3072)              199680    \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               1573376   \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,880,938\n",
      "Trainable params: 1,880,810\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/25\n",
      "50000/50000 [==============================] - 125s - loss: 1.5096 - acc: 0.4463 - val_loss: 1.3169 - val_acc: 0.5184\n",
      "Epoch 2/25\n",
      "50000/50000 [==============================] - 119s - loss: 1.2025 - acc: 0.5674 - val_loss: 1.2564 - val_acc: 0.5353\n",
      "Epoch 3/25\n",
      "50000/50000 [==============================] - 114s - loss: 1.0610 - acc: 0.6230 - val_loss: 1.1232 - val_acc: 0.5992\n",
      "Epoch 4/25\n",
      "50000/50000 [==============================] - 957s - loss: 0.9602 - acc: 0.6623 - val_loss: 1.0672 - val_acc: 0.6245\n",
      "Epoch 5/25\n",
      "50000/50000 [==============================] - 122s - loss: 0.8840 - acc: 0.6903 - val_loss: 1.0751 - val_acc: 0.6296\n",
      "Epoch 6/25\n",
      "50000/50000 [==============================] - 116s - loss: 0.8175 - acc: 0.7118 - val_loss: 1.0041 - val_acc: 0.6512\n",
      "Epoch 7/25\n",
      "50000/50000 [==============================] - 119s - loss: 0.7580 - acc: 0.7338 - val_loss: 0.9799 - val_acc: 0.6663\n",
      "Epoch 8/25\n",
      "50000/50000 [==============================] - 116s - loss: 0.7054 - acc: 0.7538 - val_loss: 1.0718 - val_acc: 0.6432\n",
      "Epoch 9/25\n",
      "50000/50000 [==============================] - 118s - loss: 0.6576 - acc: 0.7685 - val_loss: 1.0407 - val_acc: 0.6494\n",
      "Epoch 10/25\n",
      "50000/50000 [==============================] - 123s - loss: 0.6158 - acc: 0.7846 - val_loss: 1.0292 - val_acc: 0.6719\n",
      "Epoch 11/25\n",
      "50000/50000 [==============================] - 123s - loss: 0.5747 - acc: 0.7993 - val_loss: 1.0136 - val_acc: 0.6760\n",
      "Epoch 12/25\n",
      "50000/50000 [==============================] - 120s - loss: 0.5340 - acc: 0.8115 - val_loss: 1.0748 - val_acc: 0.6719\n",
      "Epoch 13/25\n",
      "50000/50000 [==============================] - 117s - loss: 0.5005 - acc: 0.8247 - val_loss: 1.1048 - val_acc: 0.6748\n",
      "Epoch 14/25\n",
      "50000/50000 [==============================] - 119s - loss: 0.4632 - acc: 0.8394 - val_loss: 1.2267 - val_acc: 0.6597\n",
      "Epoch 15/25\n",
      "50000/50000 [==============================] - 117s - loss: 0.4327 - acc: 0.8500 - val_loss: 1.1548 - val_acc: 0.6731\n",
      "Epoch 16/25\n",
      "50000/50000 [==============================] - 117s - loss: 0.4015 - acc: 0.8589 - val_loss: 1.3039 - val_acc: 0.6520\n",
      "Epoch 17/25\n",
      "50000/50000 [==============================] - 116s - loss: 0.3775 - acc: 0.8666 - val_loss: 1.3155 - val_acc: 0.6597\n",
      "Epoch 18/25\n",
      "50000/50000 [==============================] - 117s - loss: 0.3520 - acc: 0.8760 - val_loss: 1.3416 - val_acc: 0.6609\n",
      "Epoch 19/25\n",
      "50000/50000 [==============================] - 121s - loss: 0.3299 - acc: 0.8840 - val_loss: 1.4650 - val_acc: 0.6760\n",
      "Epoch 20/25\n",
      "50000/50000 [==============================] - 125s - loss: 0.3099 - acc: 0.8922 - val_loss: 1.4631 - val_acc: 0.6650\n",
      "Epoch 21/25\n",
      "50000/50000 [==============================] - 123s - loss: 0.2929 - acc: 0.8976 - val_loss: 1.5779 - val_acc: 0.6502\n",
      "Epoch 22/25\n",
      "50000/50000 [==============================] - 122s - loss: 0.2768 - acc: 0.9031 - val_loss: 1.6386 - val_acc: 0.6500\n",
      "Epoch 23/25\n",
      "50000/50000 [==============================] - 124s - loss: 0.2611 - acc: 0.9091 - val_loss: 1.8762 - val_acc: 0.6402\n",
      "Epoch 24/25\n",
      "50000/50000 [==============================] - 120s - loss: 0.2480 - acc: 0.9128 - val_loss: 1.7622 - val_acc: 0.6723\n",
      "Epoch 25/25\n",
      "50000/50000 [==============================] - 121s - loss: 0.2376 - acc: 0.9167 - val_loss: 1.7639 - val_acc: 0.6649\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f76783bc978>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "              batch_size=32,\n",
    "              epochs=25,\n",
    "              validation_data=(X_test, y_test),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9984/10000 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.7638811379432677, 0.66490000000000005]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model appears to begin offering promising results. On training data it performs extremely well, reaching accuracy of .91 and only .23 loss. However, when validation scores are considered, it does not appear to be learning the data better. In fact, val scores peak around Epoch 13, at .67.\n",
    "\n",
    "Compared to the baseline models I have considered so far, this model is not doing terrible, but there still remains a lot of room for improvement. \n",
    "\n",
    "A notable difference between this model and the example model fit in notebook one is the presence of batch normalization layers instead of dropout layers. These layers are both importation layers for preventing overfitting. \n",
    "\n",
    "Dropout layers prevent overfitting by randomly resetting a fraction of the inputs to zero at each layer. This helps by reducing the noise - think of it as decreasing the resolution or sharpness of the image - so that the true features that make up a shape can be extracted. For example, human faces are easily recognizable to us. However, no two single humans faces have the same exact shape. By randomly dropping some of the uniqueness of each input, the true features may be extracted. \n",
    "\n",
    "As a final addition to this notebook, I will now fit a variation of the AlexNet model that incorporates these dropout layers while removing the normalization layers in an attempt to produce better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3,3), padding='same', input_shape=X_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(32, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(64, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(64, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(3072))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 5, 5, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3072)              199680    \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               1573376   \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,880,682\n",
      "Trainable params: 1,880,682\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/25\n",
      "50000/50000 [==============================] - 109s - loss: 1.9266 - acc: 0.2701 - val_loss: 1.7220 - val_acc: 0.3685\n",
      "Epoch 2/25\n",
      "50000/50000 [==============================] - 111s - loss: 1.6339 - acc: 0.3964 - val_loss: 1.4856 - val_acc: 0.4584\n",
      "Epoch 3/25\n",
      "50000/50000 [==============================] - 249s - loss: 1.4901 - acc: 0.4509 - val_loss: 1.4177 - val_acc: 0.4863\n",
      "Epoch 4/25\n",
      "50000/50000 [==============================] - 503s - loss: 1.3966 - acc: 0.4902 - val_loss: 1.3967 - val_acc: 0.4896\n",
      "Epoch 5/25\n",
      "50000/50000 [==============================] - 111s - loss: 1.3229 - acc: 0.5201 - val_loss: 1.2682 - val_acc: 0.5450\n",
      "Epoch 6/25\n",
      "50000/50000 [==============================] - 110s - loss: 1.2595 - acc: 0.5473 - val_loss: 1.2389 - val_acc: 0.5611\n",
      "Epoch 7/25\n",
      "50000/50000 [==============================] - 118s - loss: 1.2115 - acc: 0.5636 - val_loss: 1.1560 - val_acc: 0.5836\n",
      "Epoch 8/25\n",
      "50000/50000 [==============================] - 117s - loss: 1.1631 - acc: 0.5840 - val_loss: 1.1840 - val_acc: 0.5772\n",
      "Epoch 9/25\n",
      "50000/50000 [==============================] - 107s - loss: 1.1246 - acc: 0.5993 - val_loss: 1.0967 - val_acc: 0.6041\n",
      "Epoch 10/25\n",
      "50000/50000 [==============================] - 116s - loss: 1.0952 - acc: 0.6103 - val_loss: 1.1055 - val_acc: 0.6078\n",
      "Epoch 11/25\n",
      "50000/50000 [==============================] - 118s - loss: 1.0668 - acc: 0.6213 - val_loss: 1.0768 - val_acc: 0.6207\n",
      "Epoch 12/25\n",
      "50000/50000 [==============================] - 114s - loss: 1.0413 - acc: 0.6297 - val_loss: 1.0220 - val_acc: 0.6404\n",
      "Epoch 13/25\n",
      "50000/50000 [==============================] - 110s - loss: 1.0157 - acc: 0.6407 - val_loss: 1.0335 - val_acc: 0.6382\n",
      "Epoch 14/25\n",
      "50000/50000 [==============================] - 110s - loss: 0.9916 - acc: 0.6482 - val_loss: 0.9772 - val_acc: 0.6562\n",
      "Epoch 15/25\n",
      "50000/50000 [==============================] - 117s - loss: 0.9749 - acc: 0.6538 - val_loss: 1.0301 - val_acc: 0.6450\n",
      "Epoch 16/25\n",
      "50000/50000 [==============================] - 115s - loss: 0.9548 - acc: 0.6629 - val_loss: 0.9668 - val_acc: 0.6592\n",
      "Epoch 17/25\n",
      "50000/50000 [==============================] - 120s - loss: 0.9396 - acc: 0.6684 - val_loss: 0.9743 - val_acc: 0.6669\n",
      "Epoch 18/25\n",
      "50000/50000 [==============================] - 114s - loss: 0.9213 - acc: 0.6754 - val_loss: 0.9340 - val_acc: 0.6726\n",
      "Epoch 19/25\n",
      "50000/50000 [==============================] - 117s - loss: 0.9088 - acc: 0.6804 - val_loss: 0.9731 - val_acc: 0.6711\n",
      "Epoch 20/25\n",
      "50000/50000 [==============================] - 114s - loss: 0.8975 - acc: 0.6841 - val_loss: 0.9333 - val_acc: 0.6811\n",
      "Epoch 21/25\n",
      "50000/50000 [==============================] - 113s - loss: 0.8888 - acc: 0.6879 - val_loss: 0.9065 - val_acc: 0.6838\n",
      "Epoch 22/25\n",
      "50000/50000 [==============================] - 114s - loss: 0.8775 - acc: 0.6911 - val_loss: 0.9936 - val_acc: 0.6659\n",
      "Epoch 23/25\n",
      "50000/50000 [==============================] - 115s - loss: 0.8628 - acc: 0.6968 - val_loss: 0.8940 - val_acc: 0.6933\n",
      "Epoch 24/25\n",
      "50000/50000 [==============================] - 115s - loss: 0.8599 - acc: 0.7008 - val_loss: 0.8879 - val_acc: 0.7003\n",
      "Epoch 25/25\n",
      "50000/50000 [==============================] - 119s - loss: 0.8471 - acc: 0.7033 - val_loss: 0.8627 - val_acc: 0.7068\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f76ccae2978>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "              batch_size=32,\n",
    "              epochs=25,\n",
    "              validation_data=(X_test, y_test),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding dropout does help the validation score slightly here, boosting it from .66 in the first iteration of the AlexNet model to just above .70 in this iteration. \n",
    "\n",
    "After reading through the paper located here http://vision.stanford.edu/teaching/cs231b_spring1415/slides/alexnet_tugce_kyunghee.pdf in greater depth, I made a few changes again to the model. Now I incorporate Normalization layers in the first two layers of the model, before applying MaxPooling instead of after. I also adjusted the size of the Fully Connected layers at the end and added to new Dropout .5 layers after each, as was suggested in order to decrease overfitting in the AlexNet published paper.\n",
    "\n",
    "By the end of the 7 convulutional layers, 64 filters have been created. Each of these 64 filters then connects to each of the fully connected neurons. The neurons learn to give different weightings to each of the 64 shape filters in order to put together ideas about what shapes form together to create certain image classifications. This process happens through two layers, and then final decisions are made. Dropout is introduced in these layers to prevent overfitting in the training model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3,3), padding='same', input_shape=X_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(32, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(64, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(64, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_14 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 14, 14, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 5, 5, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 4096)              266240    \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                40970     \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 17,191,274\n",
      "Trainable params: 17,191,146\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train,\n",
    "              batch_size=32,\n",
    "              epochs=25,\n",
    "              validation_data=(X_test, y_test),\n",
    "              shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
