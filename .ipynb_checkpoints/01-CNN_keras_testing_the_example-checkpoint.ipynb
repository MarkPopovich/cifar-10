{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 - Fitting a Convulutional Neural Network\n",
    "#### Working from example at https://github.com/fchollet/keras/blob/master/examples/cifar10_cnn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will fit a convulutional neural network on the CIFAR-10 images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (50000, 32, 32, 3), y_train: (50000,)\n",
      "X_test: (10000, 32, 32, 3), y_test: (10000,)\n",
      "Class labels: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "%run __init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be my first try at building a CNN using keras. I will rely heavily on work done by others, but will seek to experiment with many different combinations with the hope that my experimentation will lead to knowledge about how these models work.\n",
    "\n",
    "Specifically, for this notebook I will work on implementing the neural network given as an example here: https://github.com/fchollet/keras/blob/master/examples/cifar10_cnn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the labels vector y is a single vector with values ranging from zero to 9. From reading other models, I see that target should be a one hot encoding instead. \n",
    "\n",
    "The following code uses built in Keras functionality to transform the y vector into a sparse matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 9, 9, 4, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 8, 8, 0, 6])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras includes two kinds of models: Sequential and KerasFunctionalAPI. Sequential is the simpler implementation and adds model layers in a linear fashion. The FunctionalAPI is the more complex model and allows the user to create complex architectures to \"build arbitrary graphs of layers.\" (Keras documentation, https://keras.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, I will build a simple Sequential model and experiment by adding layers in a somewhat unstructured manner, with the goal of blindly training better models as I go. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding layers to the neural network is accomplished by using model.add. \n",
    "\n",
    "The first layer I will add will be a Convulutional Neural Network, 2D, for 2 dimensional image. \n",
    "\n",
    "filters=32 defines the dimensionality of the output space.\n",
    "\n",
    "kernel_size=(3,3) specifies the width and height of the 2D convolution window. \n",
    "\n",
    "padding='same' ... I'm not sure what this does. \n",
    "\n",
    "input_shape=(32,32,3) specifies the shape the data will be input. I am working with 32x32 RBG images, so the input shape is (32,32,3). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', input_shape=X_train.shape[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I will add an activation layer with a rectified linear unit, or 'ReLu'.\n",
    "\n",
    "The 'ReLu' function is defined as:\n",
    "\n",
    "f(x) = max(0,x)\n",
    "\n",
    "In this case, the function will only activate if the value of x is positive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far I have a single layer neural network. Since I am implementing neural networks here for the first time, I will compile this \"network\" and use its performance results as a baseline implementation. Afterwords I will add more layers to the next work and refit it to study model performance with complexity.\n",
    "The compiler requires an optimizer object. As in the example, I will use an RMSprop optimizer. However, I will leave all of its arguments default since I am using this as a baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: As my model was not running and I was receiving errors that were beyond my ability to debug, I opted to simply copy and paste a majority of the code below with the simple primary goal of at least getting a model to run/fit. This will not be my final model, but I look at it as simply a learning experience to implement someone elses code could as a first, blind stab at fitting a neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/25\n",
      "50000/50000 [==============================] - 174s - loss: 1.8092 - acc: 0.3359 - val_loss: 1.5419 - val_acc: 0.4469\n",
      "Epoch 2/25\n",
      "50000/50000 [==============================] - 177s - loss: 1.5218 - acc: 0.4470 - val_loss: 1.3903 - val_acc: 0.4986\n",
      "Epoch 3/25\n",
      "50000/50000 [==============================] - 173s - loss: 1.3826 - acc: 0.5037 - val_loss: 1.2976 - val_acc: 0.5360\n",
      "Epoch 4/25\n",
      "50000/50000 [==============================] - 172s - loss: 1.2831 - acc: 0.5451 - val_loss: 1.1612 - val_acc: 0.5886\n",
      "Epoch 5/25\n",
      "50000/50000 [==============================] - 171s - loss: 1.1942 - acc: 0.5767 - val_loss: 1.0840 - val_acc: 0.6191\n",
      "Epoch 6/25\n",
      "50000/50000 [==============================] - 171s - loss: 1.1240 - acc: 0.6025 - val_loss: 1.0670 - val_acc: 0.6181\n",
      "Epoch 7/25\n",
      "50000/50000 [==============================] - 177s - loss: 1.0587 - acc: 0.6254 - val_loss: 1.0180 - val_acc: 0.6463\n",
      "Epoch 8/25\n",
      "50000/50000 [==============================] - 182s - loss: 1.0093 - acc: 0.6460 - val_loss: 0.9289 - val_acc: 0.6754\n",
      "Epoch 9/25\n",
      "50000/50000 [==============================] - 177s - loss: 0.9711 - acc: 0.6587 - val_loss: 0.9125 - val_acc: 0.67976\n",
      "Epoch 10/25\n",
      "50000/50000 [==============================] - 173s - loss: 0.9317 - acc: 0.6742 - val_loss: 0.8727 - val_acc: 0.6927\n",
      "Epoch 11/25\n",
      "50000/50000 [==============================] - 183s - loss: 0.9025 - acc: 0.6842 - val_loss: 0.8815 - val_acc: 0.6927\n",
      "Epoch 12/25\n",
      "50000/50000 [==============================] - 821s - loss: 0.8748 - acc: 0.6935 - val_loss: 0.8768 - val_acc: 0.6942\n",
      "Epoch 13/25\n",
      "50000/50000 [==============================] - 184s - loss: 0.8494 - acc: 0.7035 - val_loss: 0.8146 - val_acc: 0.7123\n",
      "Epoch 14/25\n",
      "50000/50000 [==============================] - 171s - loss: 0.8309 - acc: 0.7104 - val_loss: 0.8249 - val_acc: 0.7104\n",
      "Epoch 15/25\n",
      "50000/50000 [==============================] - 172s - loss: 0.8079 - acc: 0.7197 - val_loss: 0.8255 - val_acc: 0.7136\n",
      "Epoch 16/25\n",
      "50000/50000 [==============================] - 170s - loss: 0.7904 - acc: 0.7271 - val_loss: 0.7553 - val_acc: 0.7379\n",
      "Epoch 17/25\n",
      "50000/50000 [==============================] - 181s - loss: 0.7706 - acc: 0.7332 - val_loss: 0.7641 - val_acc: 0.7330\n",
      "Epoch 18/25\n",
      "50000/50000 [==============================] - 183s - loss: 0.7612 - acc: 0.7356 - val_loss: 0.7372 - val_acc: 0.7450\n",
      "Epoch 19/25\n",
      "50000/50000 [==============================] - 170s - loss: 0.7491 - acc: 0.7408 - val_loss: 0.7542 - val_acc: 0.7374\n",
      "Epoch 20/25\n",
      "50000/50000 [==============================] - 177s - loss: 0.7412 - acc: 0.7427 - val_loss: 0.7276 - val_acc: 0.7482\n",
      "Epoch 21/25\n",
      "50000/50000 [==============================] - 183s - loss: 0.7272 - acc: 0.7499 - val_loss: 0.7335 - val_acc: 0.7472\n",
      "Epoch 22/25\n",
      "50000/50000 [==============================] - 169s - loss: 0.7195 - acc: 0.7520 - val_loss: 0.7272 - val_acc: 0.7499\n",
      "Epoch 23/25\n",
      "50000/50000 [==============================] - 168s - loss: 0.7163 - acc: 0.7536 - val_loss: 0.7259 - val_acc: 0.7512\n",
      "Epoch 24/25\n",
      "50000/50000 [==============================] - 169s - loss: 0.7024 - acc: 0.7580 - val_loss: 0.7016 - val_acc: 0.7620\n",
      "Epoch 25/25\n",
      "50000/50000 [==============================] - 184s - loss: 0.6998 - acc: 0.7605 - val_loss: 0.7437 - val_acc: 0.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7eff3316da58>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "              batch_size=32,\n",
    "              epochs=25,\n",
    "              validation_data=(X_test, y_test),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, in order to get the model to function, I ended up copying a majority of the code from the example. While I tried to implement a more barebones solution, I wasn't able to get it to run and so opted to at least get a solution that will perform.\n",
    "\n",
    "Even then, the model only fit to an accuracy score of about .10. In the first model, which is no longer part of this notebook, I left the numpy array encoded as values from 0 to 255. Changing the type to float32 and dividing by 255 to encode them as 0.0 to 1.0 values significantly boosted the model performance from .10 to .46 (.638 early on in the training epochs). Clearly, feature selection is paramount to a CNN's ability to successfully classify images. \n",
    "\n",
    "\n",
    "However, looking at the accuracy scores above, I notice that accuracy increases over the first 5 epochs of training, but surprisingly begins to decrease afterward. Could this be a vanishing gradient? \n",
    "\n",
    "My next objective will be to gain a better understand of the basics of neural networks. What are the different layers? How do they work? What, really, is back propagation? How do I begin to tune this model and boost performance? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9952/10000 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7436710353851318, 0.75]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('example_cnn.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
