{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Neural Networks\n",
    "\n",
    "In the previous notebook, I strove to implement a convulutional neural network by following an example provided on the keras github. As a first exposure to neural networks and working with Keras, that experience helped expose me to the language and structure of building keras models. Simultaneously, I strove to understand the basics of convulutional neural networks through research and instruction.\n",
    "\n",
    "Now I would like to take a step backwards and test a simple, non-convulutional neural network. This network could be considered \"feed forward\", \"dense\", or \"fully connected\". I would like to see what happens when I do a single densely connected layer, and then perhaps two or three densely connected layers. \n",
    "\n",
    "### Softmax Activation\n",
    "\n",
    "In order to make the network classify images into one of the ten classes, adding a softmax activation after the final output layer is required. Each dense node itself outputs a float value useful for regression, but useless in the case of classification. Adding softmax to the value of the output is similiar to changing a linear model to a logistic model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "%run __initremote__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(model):\n",
    "    for l in model.layers:\n",
    "        print (l.name, l.input_shape,'==>',l.output_shape)\n",
    "    print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_acc', \n",
    "                                           min_delta=0, \n",
    "                                           patience=5, \n",
    "                                           verbose=0, \n",
    "                                           mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten(input_shape=x_train.shape[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(3072))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flatten_1 (None, 32, 32, 3) ==> (None, 3072)\n",
      "dense_1 (None, 3072) ==> (None, 3072)\n",
      "dense_2 (None, 3072) ==> (None, 10)\n",
      "activation_1 (None, 10) ==> (None, 10)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3072)              9440256   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                30730     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 9,470,986\n",
      "Trainable params: 9,470,986\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "50000/50000 [==============================] - 22s 435us/step - loss: 2.9915 - acc: 0.2421 - val_loss: 2.3665 - val_acc: 0.2315\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 19s 383us/step - loss: 2.0622 - acc: 0.3028 - val_loss: 1.9833 - val_acc: 0.3100\n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 19s 383us/step - loss: 1.8749 - acc: 0.3386 - val_loss: 1.8235 - val_acc: 0.3539\n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 19s 383us/step - loss: 1.8221 - acc: 0.3595 - val_loss: 1.8393 - val_acc: 0.3537\n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 19s 383us/step - loss: 1.7983 - acc: 0.3697 - val_loss: 1.8201 - val_acc: 0.3493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f92acce2828>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "              batch_size=32,\n",
    "              epochs=5,\n",
    "              validation_data=(x_test, y_test),\n",
    "              callbacks=[early_stop]\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, using a single dense layer with neurons equal to inputs and one output layer does not produce extraordinary results. However, at 35% validation accuracy, this model is already doing much better than a simply guess. A baseline result of 10% accuracy would equate to just picking one of the ten classes and guessing every time that it will be that class (always picking truck, for example). That is not what we see in this case, however. It is clear that this kind of network is sophisticated enough to notice some features, and start to make some very naive guesses as to which class each image may belong to. \n",
    "\n",
    "This makes a lot of sense, however. By unraveling the image data into a single 3072x1 vector, I am training on nothing but a one dimensional band of nearly meaningless data. It is unlikely a human could extract any meaningful value from this row vector. Not only that, but without any activation layer, no non-linearity is being introduced to the model, meaning the current model is not but a linear classifier with 3072 features / betas corresponding to 10 output layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens when we add an activation layer to this model? Let's try it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=x_train.shape[1:]))\n",
    "model.add(Dense(3072))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flatten_2 (None, 32, 32, 3) ==> (None, 3072)\n",
      "dense_3 (None, 3072) ==> (None, 3072)\n",
      "activation_2 (None, 3072) ==> (None, 3072)\n",
      "dense_4 (None, 3072) ==> (None, 10)\n",
      "activation_3 (None, 10) ==> (None, 10)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3072)              9440256   \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                30730     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 9,470,986\n",
      "Trainable params: 9,470,986\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "50000/50000 [==============================] - 19s 388us/step - loss: 1.9062 - acc: 0.3219 - val_loss: 1.8261 - val_acc: 0.3405\n",
      "Epoch 2/100\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 1.6666 - acc: 0.4061 - val_loss: 1.6742 - val_acc: 0.3921\n",
      "Epoch 3/100\n",
      "50000/50000 [==============================] - 19s 383us/step - loss: 1.5791 - acc: 0.4412 - val_loss: 1.6234 - val_acc: 0.4158\n",
      "Epoch 4/100\n",
      "50000/50000 [==============================] - 19s 383us/step - loss: 1.5220 - acc: 0.4583 - val_loss: 1.5211 - val_acc: 0.4592\n",
      "Epoch 5/100\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 1.4714 - acc: 0.4817 - val_loss: 1.5777 - val_acc: 0.4346\n",
      "Epoch 6/100\n",
      "50000/50000 [==============================] - 19s 383us/step - loss: 1.4383 - acc: 0.4961 - val_loss: 1.4823 - val_acc: 0.4687\n",
      "Epoch 7/100\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 1.4032 - acc: 0.5067 - val_loss: 1.4381 - val_acc: 0.4920\n",
      "Epoch 8/100\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 1.3757 - acc: 0.5175 - val_loss: 1.5183 - val_acc: 0.4411\n",
      "Epoch 9/100\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 1.3478 - acc: 0.5247 - val_loss: 1.4198 - val_acc: 0.4992\n",
      "Epoch 10/100\n",
      "50000/50000 [==============================] - 19s 383us/step - loss: 1.3238 - acc: 0.5352 - val_loss: 1.4905 - val_acc: 0.4617\n",
      "Epoch 11/100\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 1.2963 - acc: 0.5454 - val_loss: 1.4192 - val_acc: 0.5023\n",
      "Epoch 12/100\n",
      "50000/50000 [==============================] - 19s 383us/step - loss: 1.2760 - acc: 0.5518 - val_loss: 1.4246 - val_acc: 0.4994\n",
      "Epoch 13/100\n",
      "50000/50000 [==============================] - 19s 383us/step - loss: 1.2527 - acc: 0.5607 - val_loss: 1.5653 - val_acc: 0.4600\n",
      "Epoch 14/100\n",
      "50000/50000 [==============================] - 19s 383us/step - loss: 1.2341 - acc: 0.5704 - val_loss: 1.4338 - val_acc: 0.4972\n",
      "Epoch 15/100\n",
      "50000/50000 [==============================] - 19s 383us/step - loss: 1.2148 - acc: 0.5750 - val_loss: 1.4156 - val_acc: 0.4961\n",
      "Epoch 16/100\n",
      "50000/50000 [==============================] - 19s 383us/step - loss: 1.1905 - acc: 0.5847 - val_loss: 1.4528 - val_acc: 0.5003\n",
      "Epoch 17/100\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 1.1724 - acc: 0.5905 - val_loss: 1.3889 - val_acc: 0.5171\n",
      "Epoch 18/100\n",
      "50000/50000 [==============================] - 19s 383us/step - loss: 1.1535 - acc: 0.5988 - val_loss: 1.4266 - val_acc: 0.5108\n",
      "Epoch 19/100\n",
      "50000/50000 [==============================] - 19s 383us/step - loss: 1.1340 - acc: 0.6067 - val_loss: 1.4470 - val_acc: 0.5113\n",
      "Epoch 20/100\n",
      "50000/50000 [==============================] - 19s 383us/step - loss: 1.1126 - acc: 0.6121 - val_loss: 1.4135 - val_acc: 0.5125\n",
      "Epoch 21/100\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 1.0924 - acc: 0.6189 - val_loss: 1.6560 - val_acc: 0.4518\n",
      "Epoch 22/100\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 1.0764 - acc: 0.6259 - val_loss: 1.4300 - val_acc: 0.5185\n",
      "Epoch 23/100\n",
      "50000/50000 [==============================] - 19s 383us/step - loss: 1.0554 - acc: 0.6308 - val_loss: 1.5560 - val_acc: 0.4823\n",
      "Epoch 24/100\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 1.0361 - acc: 0.6395 - val_loss: 1.4101 - val_acc: 0.5287\n",
      "Epoch 25/100\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 1.0198 - acc: 0.6452 - val_loss: 1.4050 - val_acc: 0.5282\n",
      "Epoch 26/100\n",
      "50000/50000 [==============================] - 19s 385us/step - loss: 1.0016 - acc: 0.6518 - val_loss: 1.5190 - val_acc: 0.4988\n",
      "Epoch 27/100\n",
      "50000/50000 [==============================] - 19s 385us/step - loss: 0.9821 - acc: 0.6580 - val_loss: 1.4171 - val_acc: 0.5257\n",
      "Epoch 28/100\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 0.9658 - acc: 0.6646 - val_loss: 1.4008 - val_acc: 0.5369\n",
      "Epoch 29/100\n",
      "50000/50000 [==============================] - 19s 383us/step - loss: 0.9475 - acc: 0.6712 - val_loss: 1.6965 - val_acc: 0.4768\n",
      "Epoch 30/100\n",
      "50000/50000 [==============================] - 19s 385us/step - loss: 0.9326 - acc: 0.6750 - val_loss: 1.5663 - val_acc: 0.5037\n",
      "Epoch 31/100\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 0.9163 - acc: 0.6800 - val_loss: 1.4940 - val_acc: 0.5188\n",
      "Epoch 32/100\n",
      "50000/50000 [==============================] - 19s 383us/step - loss: 0.9002 - acc: 0.6862 - val_loss: 1.4340 - val_acc: 0.5267\n",
      "Epoch 33/100\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 0.8787 - acc: 0.6964 - val_loss: 1.4984 - val_acc: 0.5222\n",
      "Epoch 34/100\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 0.8630 - acc: 0.7004 - val_loss: 1.5701 - val_acc: 0.5061\n",
      "Epoch 35/100\n",
      "50000/50000 [==============================] - 19s 383us/step - loss: 0.8473 - acc: 0.7055 - val_loss: 1.5648 - val_acc: 0.5088\n",
      "Epoch 36/100\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 0.8340 - acc: 0.7091 - val_loss: 1.7059 - val_acc: 0.4973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f92ab257208>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "              batch_size=32,\n",
    "              epochs=100,\n",
    "              validation_data=(x_test, y_test),\n",
    "              callbacks=[early_stop],\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incredible. With just a single hidden layer, a Rectified Linear Unit activation layer, an output layer and a softmax activation layer, the neural network built above reaches 50% accuracy on validation in 12 epochs. \n",
    "\n",
    "Would adding a second fully connected layer help? What if the second layer has less neurons, allowing the model to start to build some selectivity into the feature input layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=x_train.shape[1:]))\n",
    "model.add(Dense(3072))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flatten_3 (None, 32, 32, 3) ==> (None, 3072)\n",
      "dense_5 (None, 3072) ==> (None, 3072)\n",
      "activation_4 (None, 3072) ==> (None, 3072)\n",
      "dense_6 (None, 3072) ==> (None, 512)\n",
      "activation_5 (None, 512) ==> (None, 512)\n",
      "dense_7 (None, 512) ==> (None, 10)\n",
      "activation_6 (None, 10) ==> (None, 10)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3072)              9440256   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               1573376   \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 11,018,762\n",
      "Trainable params: 11,018,762\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "50000/50000 [==============================] - 22s 441us/step - loss: 1.3919 - acc: 0.5086 - val_loss: 1.5010 - val_acc: 0.4646\n",
      "Epoch 2/100\n",
      "50000/50000 [==============================] - 22s 439us/step - loss: 1.3527 - acc: 0.5210 - val_loss: 1.5845 - val_acc: 0.4509\n",
      "Epoch 3/100\n",
      "50000/50000 [==============================] - 22s 440us/step - loss: 1.3146 - acc: 0.5363 - val_loss: 1.3938 - val_acc: 0.5094\n",
      "Epoch 4/100\n",
      "50000/50000 [==============================] - 22s 440us/step - loss: 1.2817 - acc: 0.5494 - val_loss: 1.4420 - val_acc: 0.4887\n",
      "Epoch 5/100\n",
      "50000/50000 [==============================] - 22s 440us/step - loss: 1.2504 - acc: 0.5588 - val_loss: 1.3413 - val_acc: 0.5309\n",
      "Epoch 6/100\n",
      "50000/50000 [==============================] - 22s 440us/step - loss: 1.2195 - acc: 0.5703 - val_loss: 1.4277 - val_acc: 0.5025\n",
      "Epoch 7/100\n",
      "50000/50000 [==============================] - 22s 440us/step - loss: 1.1907 - acc: 0.5826 - val_loss: 1.3554 - val_acc: 0.5237\n",
      "Epoch 8/100\n",
      "50000/50000 [==============================] - 22s 440us/step - loss: 1.1599 - acc: 0.5901 - val_loss: 1.4466 - val_acc: 0.5112\n",
      "Epoch 9/100\n",
      "50000/50000 [==============================] - 22s 440us/step - loss: 1.1335 - acc: 0.6017 - val_loss: 1.4507 - val_acc: 0.5019\n",
      "Epoch 10/100\n",
      "50000/50000 [==============================] - 22s 440us/step - loss: 1.1027 - acc: 0.6119 - val_loss: 1.4190 - val_acc: 0.5219\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f92842f48d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "              batch_size=32,\n",
    "              epochs=100,\n",
    "              validation_data=(x_test, y_test),\n",
    "              callbacks=[early_stop],\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, adding a single fully connected layer didn't significantly improve model performance.\n",
    "\n",
    "Luckily, Convulutional Neural Networks have proven to be extremely effective at image classification. In the next notebook, I will start to build some historically viable CNN's that have proven useful in the past."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if I don't flatten the data before sending it into the network? How would a fully connected network do if the data is flattened after the first layer?\n",
    "\n",
    "Let's test it out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_8 (None, 32, 32, 3) ==> (None, 32, 32, 64)\n",
      "activation_7 (None, 32, 32, 64) ==> (None, 32, 32, 64)\n",
      "flatten_4 (None, 32, 32, 64) ==> (None, 65536)\n",
      "dense_9 (None, 65536) ==> (None, 32)\n",
      "activation_8 (None, 32) ==> (None, 32)\n",
      "dense_10 (None, 32) ==> (None, 10)\n",
      "activation_9 (None, 10) ==> (None, 10)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 65536)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                2097184   \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 2,097,770\n",
      "Trainable params: 2,097,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "50000/50000 [==============================] - 12s 238us/step - loss: 1.9252 - acc: 0.3119 - val_loss: 1.7515 - val_acc: 0.3824\n",
      "Epoch 2/100\n",
      "50000/50000 [==============================] - 12s 235us/step - loss: 1.6878 - acc: 0.4139 - val_loss: 1.6660 - val_acc: 0.4122\n",
      "Epoch 3/100\n",
      "50000/50000 [==============================] - 12s 235us/step - loss: 1.6121 - acc: 0.4394 - val_loss: 1.5805 - val_acc: 0.4499\n",
      "Epoch 4/100\n",
      "50000/50000 [==============================] - 12s 234us/step - loss: 1.5646 - acc: 0.4562 - val_loss: 1.5787 - val_acc: 0.4412\n",
      "Epoch 5/100\n",
      "50000/50000 [==============================] - 12s 234us/step - loss: 1.5285 - acc: 0.4692 - val_loss: 1.5230 - val_acc: 0.4631\n",
      "Epoch 6/100\n",
      "50000/50000 [==============================] - 12s 235us/step - loss: 1.5007 - acc: 0.4812 - val_loss: 1.4992 - val_acc: 0.4749\n",
      "Epoch 7/100\n",
      "50000/50000 [==============================] - 12s 234us/step - loss: 1.4777 - acc: 0.4883 - val_loss: 1.4901 - val_acc: 0.4801\n",
      "Epoch 8/100\n",
      "50000/50000 [==============================] - 12s 235us/step - loss: 1.4593 - acc: 0.4957 - val_loss: 1.4883 - val_acc: 0.4773\n",
      "Epoch 9/100\n",
      "50000/50000 [==============================] - 12s 234us/step - loss: 1.4446 - acc: 0.5010 - val_loss: 1.4872 - val_acc: 0.4804\n",
      "Epoch 10/100\n",
      "50000/50000 [==============================] - 12s 234us/step - loss: 1.4300 - acc: 0.5068 - val_loss: 1.4642 - val_acc: 0.4901\n",
      "Epoch 11/100\n",
      "50000/50000 [==============================] - 12s 234us/step - loss: 1.4161 - acc: 0.5106 - val_loss: 1.4583 - val_acc: 0.4910\n",
      "Epoch 12/100\n",
      "50000/50000 [==============================] - 12s 234us/step - loss: 1.4048 - acc: 0.5150 - val_loss: 1.4694 - val_acc: 0.4901\n",
      "Epoch 13/100\n",
      "50000/50000 [==============================] - 12s 235us/step - loss: 1.3956 - acc: 0.5191 - val_loss: 1.4553 - val_acc: 0.4909\n",
      "Epoch 14/100\n",
      "50000/50000 [==============================] - 12s 234us/step - loss: 1.3855 - acc: 0.5206 - val_loss: 1.4456 - val_acc: 0.4923\n",
      "Epoch 15/100\n",
      "50000/50000 [==============================] - 12s 234us/step - loss: 1.3764 - acc: 0.5251 - val_loss: 1.4532 - val_acc: 0.4892\n",
      "Epoch 16/100\n",
      "50000/50000 [==============================] - 12s 234us/step - loss: 1.3677 - acc: 0.5290 - val_loss: 1.4371 - val_acc: 0.4998\n",
      "Epoch 17/100\n",
      "50000/50000 [==============================] - 12s 234us/step - loss: 1.3602 - acc: 0.5316 - val_loss: 1.4387 - val_acc: 0.4943\n",
      "Epoch 18/100\n",
      "50000/50000 [==============================] - 12s 235us/step - loss: 1.3527 - acc: 0.5341 - val_loss: 1.4297 - val_acc: 0.5024\n",
      "Epoch 19/100\n",
      "50000/50000 [==============================] - 12s 235us/step - loss: 1.3459 - acc: 0.5377 - val_loss: 1.4306 - val_acc: 0.4978\n",
      "Epoch 20/100\n",
      "50000/50000 [==============================] - 12s 234us/step - loss: 1.3382 - acc: 0.5389 - val_loss: 1.4278 - val_acc: 0.5032\n",
      "Epoch 21/100\n",
      "50000/50000 [==============================] - 12s 235us/step - loss: 1.3321 - acc: 0.5416 - val_loss: 1.4318 - val_acc: 0.5051\n",
      "Epoch 22/100\n",
      "50000/50000 [==============================] - 12s 234us/step - loss: 1.3258 - acc: 0.5434 - val_loss: 1.4463 - val_acc: 0.4928\n",
      "Epoch 23/100\n",
      "50000/50000 [==============================] - 12s 235us/step - loss: 1.3195 - acc: 0.5456 - val_loss: 1.4393 - val_acc: 0.4984\n",
      "Epoch 24/100\n",
      "50000/50000 [==============================] - 12s 234us/step - loss: 1.3136 - acc: 0.5484 - val_loss: 1.4305 - val_acc: 0.4993\n",
      "Epoch 25/100\n",
      "50000/50000 [==============================] - 12s 234us/step - loss: 1.3064 - acc: 0.5506 - val_loss: 1.4320 - val_acc: 0.5028\n",
      "Epoch 26/100\n",
      "50000/50000 [==============================] - 12s 234us/step - loss: 1.3014 - acc: 0.5524 - val_loss: 1.4278 - val_acc: 0.5039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f92acafe5c0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "              batch_size=32,\n",
    "              epochs=100,\n",
    "              validation_data=(x_test, y_test),\n",
    "              callbacks=[early_stop],\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network does just as well as the more complicated fully connected networks above. The use of less nodes in the hidden layers does not impare model performance, and validation scores top out at about 50%, as the previous models have before. \n",
    "\n",
    "Next, let's look at Convulutional2D layers and see how they can help the neural net classify images by creating filters and pools features together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
