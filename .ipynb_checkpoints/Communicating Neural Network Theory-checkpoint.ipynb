{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding how Convulutional Neural Networks work\n",
    "\n",
    "#### Objective: To clearly elucidate the functionality of several different Keras layers\n",
    "\n",
    "Neural Networks are complex in a variety of different ways. From their basic architecture to the instricacies behind each unqiue layer, to the optimization functions and input variables, Neural's Network represent a leap in complexity from other machine learning techniques such as linear and logistic models. \n",
    "\n",
    "Despite these often grand and confusing levels of complexity, at their simplist level neurals networks are really just those very same linear and logistic models. Indeed, the best way to understand conceptually how these networks work may not be from the biological (and popular) perspective, but from the linear model perspective that undergrids their mathematics and functioning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The First Kind of Layer: Fully Connected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplist and easiest layer to understand is the fully connected layer. This layer is called ```Dense``` in Keras, and is itself a powerful layer for problem solving. \n",
    "\n",
    "The dense layer receives inputs into a single layer of input neurons. Each input is multiplied by a weight, and then all of the inputs * weights are summed together to create a single value. If we thought of a fully connected layer with only a single neuron, this would equate to nothing more than our original linear model function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = \\beta_0 X_0 + \\beta_1 X_1 + ... + \\beta_i X_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have used linear regressions to build equations that represent a best fit line. However, what about for classification problems? When we are not looking for an output that is linear but categorical, we manipulate the output using some type of transformation. Most commonly, we use the logistic transformation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$logistic(a) = \\frac{e^{a}}{e^{a} + 1}$$ \n",
    "\n",
    "\n",
    "$$P(y=1) = \\frac{e^{\\left(\\beta_0 + \\sum_{j}^p\\beta_jx_j\\right)}}{e^{\\left(\\beta_0 + \\sum_{j}^p\\beta_jx_j\\right)}+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a single layer, single neuron network here. We could use this to potentially perform some type of simple logistic classification into one of two classes (0 or 1). We might visualize this network as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Rosenblattperceptron.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we are trying to understand patterns that are more complex than a simple line or binary classification? What if we stack these linear/logistic models next to each other, and allow the Betas of each input to vary as we seek to understand different aspects of our output? \n",
    "\n",
    "For example, in this project we seek to classify images into 1 of 10 categories. Ten output neurons are aligned into an output layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](multilayerperceptron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the linear models, followed by sigmoid activations, stacking together. Each neuron in the hidden layer is receiving some input $L_i * \\beta_i$ for each $i$. An important note is that the $beta_i$ that multiplies each $L_i$ differs with each neuron and each input. This allows every $E(y|X) = \\beta_0 + \\sum_{j}^p\\beta_jx_j$ to result in a different value. \n",
    "\n",
    "From this understanding, we can think of a neural layer as simply a stack of logistic models with depth equal to the number of neurons in the layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation: Solving the logistic by finding minimum loss\n",
    "#### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving the logistic regression in this case means searching for a set of weights, or $\\beta$s, that create the best predictions for each class. If we isolate the problem to a single output neuron, this can be thought of as adjusting the weights so that they most accurately classify each category either 'is' or 'is not' with a high degree of probability. \n",
    "\n",
    "This can be accomplished with a procedure known as gradient descent. \n",
    "\n",
    "In gradient descent, the instantaneous slope of the loss function is calculated and incremental adjustments are made in the direction of the steepest slope in an attempt to minimize the loss on any given pass. These step-wise adjustments slowly change the weighting so that the value of the loss function slowly decreases. Since the loss function can be thought of as calculating the distance of the predictions from their true values, as the loss decreases the accuracy of the model will increase.\n",
    "\n",
    "Now we apply this incremental adjustment procedure over the entire network, which in this case is a simple ten-output network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Second Kind of Layer: Convulutional 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
